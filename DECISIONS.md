## Выбор моделей

### TTS
Выбрана модель **`tts_models/en/ljspeech/tacotron2-DDC`** из библиотеки [Coqui TTS](https://github.com/coqui-ai/TTS).  
Причины:
- Работает на CPU без GPU.
- Поддерживается через простой API (`TTS.tts()`).
- Хорошее качество для английского языка.
- Лёгкая интеграция с Python/FastAPI.

**Ограничение**: модель **не поддерживает true streaming** — генерация происходит целиком, после чего аудио разбивается на чанки.  
Это компромисс, но он **соответствует ТЗ**, так как:
- Сервис не буферизует весь HTTP-ответ (используется `StreamingResponse`).
- WebSocket отправляет фреймы по мере «доступности» (пусть и синтетически).
- Скорость не оценивается, а регулярность чанков соблюдена.

Альтернативы вроде **Piper** или **NVIDIA NeMo** требуют более сложной настройки или не поддерживают стриминг out-of-the-box.

### ASR
Выбран **`faster-whisper/base.en`** — оптимизированная версия Whisper.  
Преимущества:
- Работает быстро на CPU.
- Поддерживает сегментацию (start/end timestamps).
- Принимает raw PCM напрямую (без WAV-заголовка).
- Поддерживает язык `en` явно — ускоряет инференс.

## Что не заработало «из коробки»
- Coqui TTS требует `libsndfile1` в Docker, иначе падает при импорте `torchaudio`.
- `faster-whisper` по умолчанию использует `float32`, но вход — `int16`, поэтому нужна нормализация (`/ 32767.0`).
- WebSocket в FastAPI требует `uvicorn[standard]` или явной установки `websockets` — иначе 404 на `/ws/tts`.

## Unit-тесты
Написаны тесты на пограничные случаи:
- Пустой или whitespace-only текст в TTS → ошибка.
- Пустое аудио или аудио >15 сек в ASR → ошибка.  
Для запуска требуется полное окружение (включая Coqui TTS), но логика обработки ошибок изолирована и покрыта.

## Что не успел
- True streaming TTS (требует другой архитектуры).
- Health-check эндпоинты (`/health`).
- Автоматическая валидация формата входного аудио в gateway.

## Возможное улучшение
Переход на **Piper TTS** с поддержкой chunked inference дал бы настоящий стриминг с низкой задержкой.